# Tokenization Newsletter Configuration

# Search keywords for finding relevant papers/posts
keywords:
  primary:
    - "tokenization"
    - "tokenizer"
    - "tokenizers"
    - "byte-pair encoding"
    - "BPE"
    - "wordpiece"
    - "sentencepiece"
    - "unigram tokenization"
  secondary:
    # These are used in combination with primary keywords for filtering
    - "language model"
    - "LLM"
    - "transformer"
    - "vocabulary"
    - "subword"
    - "detokenization"
    - "token embedding"
    - "token boundary"
    - "morphological"
    - "multilingual"

# Arxiv categories to search
arxiv:
  categories:
    - "cs.CL"
    - "cs.LG"
    - "cs.AI"
  max_results_per_query: 50

# Semantic Scholar
semantic_scholar:
  max_results_per_query: 30
  fields_of_study:
    - "Computer Science"

# Hugging Face blog
huggingface:
  blog_rss: "https://huggingface.co/blog/feed.xml"

# Google Scholar
google_scholar:
  # Google Scholar alert RSS feeds (user needs to set these up manually)
  # Go to scholar.google.com -> Create Alert -> get RSS feed URL
  alert_feeds: []

# Twitter/X accounts to monitor (manual for now - API is expensive)
twitter:
  accounts_to_check:
    - "kaboroevich"     # Andrej Karpathy
    - "sanderland_"     # Sander Land
    - "huggingface"

# Claude API settings
claude:
  model: "claude-sonnet-4-5-20250514"
  max_tokens_summary: 300
  max_tokens_editorial: 1000

# Newsletter settings
newsletter:
  name: "Tokenization Digest"
  tagline: "Monthly updates from the world of LLM tokenization"
  max_items: 10  # max papers/posts per issue
  lookback_days: 35  # how far back to search (slightly over a month for overlap)
